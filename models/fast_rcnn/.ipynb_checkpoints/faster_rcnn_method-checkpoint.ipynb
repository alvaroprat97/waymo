{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This document serves to provide the bounding boxes using MaskRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import functools\n",
    "from torchvision import transforms\n",
    "from dataset import WaymoDataset, Resizer, Normalizer, Augmenter, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\"batch_size\": 2,\n",
    "                       \"shuffle\": True,\n",
    "                       \"drop_last\": True,\n",
    "                       \"collate_fn\": collate_fn,\n",
    "                       \"num_workers\": 4}\n",
    "\n",
    "training_set = WaymoDataset(\n",
    "        cameras=['FRONT'], scope='training',\n",
    "        transform=transforms.Compose([Normalizer(), Resizer()]), \n",
    "        mod='fast_rcnn')\n",
    "\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(training_set)).tolist()\n",
    "training_set = torch.utils.data.Subset(training_set, indices[:50])\n",
    "\n",
    "training_generator = DataLoader(training_set, **training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = {\"batch_size\": 10,\n",
    "               \"shuffle\": False,\n",
    "               \"drop_last\": False,\n",
    "               \"collate_fn\": collate_fn,\n",
    "               \"num_workers\": 4}\n",
    "\n",
    "test_set = WaymoDataset(\n",
    "    cameras=['FRONT'], scope='validation',\n",
    "    transform=transforms.Compose([Normalizer(), Resizer()]),\n",
    "    mod='fast_rcnn')\n",
    "test_generator = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for da in test_generator:\n",
    "    print(da)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 4\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "\n",
    "# Granular choice for all the different layers we could freeze in the Faster RCNNs using ResNets.\n",
    "# {'backbone':{'body':['conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4'],\n",
    "#              'fpn':['inner_blocks', 'layer_blocks', 'extra_blocks']}\n",
    "#  'rpn':{'anchor_generator':'', 'head':['conv', 'cls_logits', 'bbox_pred']}\n",
    "#  'roi_heads':{'box_roi_pool', 'box_head':['fc6', 'fc7'], 'box_predictor':['cls_score', 'bbox_pred']}}\n",
    " \n",
    "#  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def transfer_learning(model, layer_parts = ['backbone', 'rpn', 'roi_heads.box_head.fc6']):\n",
    "    print(f\"Num trainable params before:{count_parameters(model)}\")\n",
    "    layers_to_freeze = []\n",
    "    text = ''\n",
    "    for layer_part in layer_parts:\n",
    "        layers_to_freeze.append(rgetattr(model, layer_part))\n",
    "        text += layer_part + ', '\n",
    "    for layer_group in layers_to_freeze:\n",
    "        for layer in layer_group.modules():\n",
    "            freeze_params(layer)\n",
    "    print(f\"Num trainable params after:{count_parameters(model)}\")\n",
    "    print(f\"Layers: {text} are frozen.\")\n",
    "\n",
    "def freeze_params(layer):\n",
    "    for parameter in layer.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            parameter.requires_grad = False\n",
    "            \n",
    "def rgetattr(obj, attr):\n",
    "    def _getattr(obj, attr):\n",
    "        return getattr(obj, attr)\n",
    "    return functools.reduce(_getattr, [obj] + attr.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# get the model using our helper function\n",
    "# model = get_instance_segmentation_model(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "output = model([training_set[0][0]], [training_set[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter_per_epoch = len(training_generator)\n",
    "test_interval = 4\n",
    "num_epochs = 2\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    # if torch.cuda.is_available():\n",
    "    #     model.module.freeze_bn()\n",
    "    # else:\n",
    "    #     model.freeze_bn()\n",
    "    epoch_loss = []\n",
    "    progress_bar = tqdm(training_generator)\n",
    "    for iter, data in enumerate(progress_bar):\n",
    "        optimizer.zero_grad()\n",
    "        images = data[0]\n",
    "        targets = data[1]\n",
    "        images = list(image for image in images)\n",
    "        targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "#             losses = model([data['img'].cuda().float(), data['annot'].cuda()])\n",
    "#             losses = model(data[0].cuda(), data[1].cuda())\n",
    "            losses = model(images.cuda(), targets.cuda())\n",
    "            cls_loss, reg_loss = losses['loss_classifier'], losses['loss_box_reg']\n",
    "        else:\n",
    "#             losses = model([data['img'].float(), data['annot']])\n",
    "#             losses = model(data[0], data[1])\n",
    "            losses = model(images, targets)\n",
    "            cls_loss, reg_loss = losses['loss_classifier'], losses['loss_box_reg']\n",
    "\n",
    "        cls_loss = cls_loss.mean()\n",
    "        reg_loss = reg_loss.mean()\n",
    "        loss = cls_loss + reg_loss\n",
    "        if loss == 0:\n",
    "            continue\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(float(loss))\n",
    "        total_loss = np.mean(epoch_loss)\n",
    "        if iter % 5 == 0:\n",
    "            print(f'Total loss at iteration {iter}: {total_loss}')\n",
    "        progress_bar.set_description(\n",
    "            'Epoch: {}/{}. Iteration: {}/{}. Cls loss: {:.5f}. Reg loss: {:.5f}. Batch loss: {:.5f} Total loss: {:.5f}'.format(\n",
    "                epoch + 1, num_epochs, iter + 1, num_iter_per_epoch, cls_loss, reg_loss, loss,\n",
    "                total_loss))\n",
    "        # writer.add_scalar('Train/Total_loss', total_loss, epoch * num_iter_per_epoch + iter)\n",
    "        # writer.add_scalar('Train/Regression_loss', reg_loss, epoch * num_iter_per_epoch + iter)\n",
    "        # writer.add_scalar('Train/Classfication_loss (focal loss)', cls_loss, epoch * num_iter_per_epoch + iter)\n",
    "        # Save every 100 samples\n",
    "#             if iter % 200 ==0:\n",
    "#                 print(f\"Saving model at :{opt.saved_path}/effijklkjcientdet_waymo.pth\")\n",
    "#                 torch.save(model, os.path.join(opt.saved_path, \"effickjhghjkientdet_waymo.pth\"))\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 continue\n",
    "    scheduler.step(np.mean(epoch_loss))\n",
    "\n",
    "    if epoch % test_interval == 0:\n",
    "        loss_regression_ls = []\n",
    "        loss_classification_ls = []\n",
    "        for iter, data in enumerate(test_generator):\n",
    "            with torch.no_grad():\n",
    "                images = data[0]\n",
    "                targets = data[1]\n",
    "                images = list(image for image in images)\n",
    "                targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    losses = model(images.cuda(), targets.cuda())\n",
    "                    cls_loss, reg_loss = losses['loss_classifier'], losses['loss_box_reg']\n",
    "                else:\n",
    "                    losses = model(images, targets)\n",
    "                    cls_loss, reg_loss = losses['loss_classifier'], losses['loss_box_reg']\n",
    "\n",
    "                cls_loss = cls_loss.mean()\n",
    "                reg_loss = reg_loss.mean()\n",
    "\n",
    "                loss_classification_ls.append(float(cls_loss))\n",
    "                loss_regression_ls.append(float(reg_loss))\n",
    "\n",
    "        cls_loss = np.mean(loss_classification_ls)\n",
    "        reg_loss = np.mean(loss_regression_ls)\n",
    "        loss = cls_loss + reg_loss\n",
    "\n",
    "        print(\n",
    "            'Epoch: {}/{}. Classification loss: {:1.5f}. Regression loss: {:1.5f}. Total loss: {:1.5f}'.format(\n",
    "                epoch + 1, num_epochs, cls_loss, reg_loss,\n",
    "                np.mean(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_regression_ls = []\n",
    "loss_classification_ls = []\n",
    "for iter, data in enumerate(test_generator):\n",
    "    with torch.no_grad():\n",
    "        images = data[0]\n",
    "        targets = data[1]\n",
    "        images = list(image for image in images)\n",
    "        targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            losses = model(images.cuda(), targets.cuda())\n",
    "            cls_loss, reg_loss = losses['loss_classifier'], losses['loss_box_reg']\n",
    "        else:\n",
    "            losses = model(images, targets)\n",
    "            cls_loss, reg_loss = losses['loss_classifier'], losses['loss_box_reg']\n",
    "\n",
    "        cls_loss = cls_loss.mean()\n",
    "        reg_loss = reg_loss.mean()\n",
    "\n",
    "        loss_classification_ls.append(float(cls_loss))\n",
    "        loss_regression_ls.append(float(reg_loss))\n",
    "\n",
    "cls_loss = np.mean(loss_classification_ls)\n",
    "reg_loss = np.mean(loss_regression_ls)\n",
    "loss = cls_loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inference\n",
    "model.eval()\n",
    "x = [test_set[2][0]]\n",
    "predictions = model(x)           # Returns predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]['boxes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples to extract the layers and their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, layer) in model._modules.items():\n",
    "    #iteration over outer layers\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "for (name, layer) in resnet._modules.items():\n",
    "    #iteration over outer layers\n",
    "    print((name))\n",
    "\n",
    "# resnet._modules['layer1'][0]._modules['bn1'].weight.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "dataset = WaymoDataset(cameras=['FRONT'], heatmaps=False)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    " collate_fn=utils.collate_fn)\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use our dataset and defined transformations\n",
    "# dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "# dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:1])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-1:])\n",
    "\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "# assign 1 worker\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
