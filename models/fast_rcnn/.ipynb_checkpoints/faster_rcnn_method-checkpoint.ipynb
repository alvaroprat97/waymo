{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This document serves to provide the bounding boxes using MaskRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import functools\n",
    "from torchvision import transforms\n",
    "from dataset import WaymoDataset, Resizer, Normalizer, Augmenter, collater, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb3f70ce210>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\"batch_size\": 2,\n",
    "                       \"shuffle\": True,\n",
    "                       \"drop_last\": True,\n",
    "                       \"collate_fn\": collate_fn,\n",
    "                       \"num_workers\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = WaymoDataset(\n",
    "        cameras=['FRONT'],scope='training',\n",
    "        transform=transforms.Compose([Normalizer(), Resizer()]), \n",
    "        exclusions=True, mod='fast_rcnn')\n",
    "\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(training_set)).tolist()\n",
    "training_set = torch.utils.data.Subset(training_set, indices[:50])\n",
    "\n",
    "training_generator = DataLoader(training_set, **training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/project_x/data/validation/FRONT_with_objects.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3574823e72ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcameras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FRONT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fast_rcnn'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/project_x/models/fast_rcnn/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, scope, root_dir, cameras, exclusions, transform, mod)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fast_rcnn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcameras\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepaths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'/{cam}_with_objects.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/project_x/models/fast_rcnn/dataset.py\u001b[0m in \u001b[0;36mload_pickle\u001b[0;34m(fpath)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/project_x/data/validation/FRONT_with_objects.pickle'"
     ]
    }
   ],
   "source": [
    "test_params = {\"batch_size\": 2,\n",
    "               \"shuffle\": False,\n",
    "               \"drop_last\": False,\n",
    "               \"collate_fn\": collate_fn,\n",
    "               \"num_workers\": 4}\n",
    "\n",
    "test_set = WaymoDataset(\n",
    "    cameras=['FRONT'], scope='validation',\n",
    "    transform=transforms.Compose([Normalizer(), Resizer()]),\n",
    "    mod='fast_rcnn'\n",
    ")\n",
    "test_generator = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 4\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "\n",
    "# Granular choice for all the different layers we could freeze in the Faster RCNNs using ResNets.\n",
    "# {'backbone':{'body':['conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4'],\n",
    "#              'fpn':['inner_blocks', 'layer_blocks', 'extra_blocks']}\n",
    "#  'rpn':{'anchor_generator':'', 'head':['conv', 'cls_logits', 'bbox_pred']}\n",
    "#  'roi_heads':{'box_roi_pool', 'box_head':['fc6', 'fc7'], 'box_predictor':['cls_score', 'bbox_pred']}}\n",
    " \n",
    "#  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def transfer_learning(model, layer_parts = ['backbone', 'rpn', 'roi_heads.box_head.fc6']):\n",
    "    print(f\"Num trainable params before:{count_parameters(model)}\")\n",
    "    layers_to_freeze = []\n",
    "    text = ''\n",
    "    for layer_part in layer_parts:\n",
    "        layers_to_freeze.append(rgetattr(model, layer_part))\n",
    "        text += layer_part + ', '\n",
    "    for layer_group in layers_to_freeze:\n",
    "        for layer in layer_group.modules():\n",
    "            freeze_params(layer)\n",
    "    print(f\"Num trainable params after:{count_parameters(model)}\")\n",
    "    print(f\"Layers: {text} are frozen.\")\n",
    "\n",
    "def freeze_params(layer):\n",
    "    for parameter in layer.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            parameter.requires_grad = False\n",
    "            \n",
    "def rgetattr(obj, attr):\n",
    "    def _getattr(obj, attr):\n",
    "        return getattr(obj, attr)\n",
    "    return functools.reduce(_getattr, [obj] + attr.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num trainable params before:41087011\n",
      "Num trainable params after:1070100\n",
      "Layers: backbone, rpn, roi_heads.box_head.fc6,  are frozen.\n"
     ]
    }
   ],
   "source": [
    "transfer_learning(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# get the model using our helper function\n",
    "# model = get_instance_segmentation_model(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[307.3738, 160.1712, 310.7423, 171.2872],\n",
       "         [  0.0000, 195.1192,  20.2951, 226.2776],\n",
       "         [ 81.4371, 169.8828, 108.2156, 186.2752],\n",
       "         [  0.0000, 176.1715,  15.8319, 184.5927],\n",
       "         [279.3312, 161.4344, 289.2682, 169.0135],\n",
       "         [300.3000, 159.8344, 305.6896, 170.9504],\n",
       "         [308.3843, 160.1712, 313.7739, 171.2872],\n",
       "         [158.6554, 168.5924, 173.8136, 183.7506],\n",
       "         [228.6356, 162.6976, 236.2147, 170.7819],\n",
       "         [266.1941, 159.6660, 276.1311, 168.0872],\n",
       "         [151.8342, 172.1293, 165.4766, 184.9296],\n",
       "         [ 56.6799, 175.2031,  59.5425, 183.7073],\n",
       "         [335.0796, 160.3396, 364.8906, 176.8452],\n",
       "         [278.5733, 161.6028, 284.2997, 168.1714],\n",
       "         [355.2062, 157.7291, 363.6274, 160.5923],\n",
       "         [104.6755, 169.9398, 118.3179, 179.3716],\n",
       "         [ 20.7162, 177.6031,  34.8638, 207.4142],\n",
       "         [ 54.4476, 177.1600, 105.6469, 209.0335],\n",
       "         [349.0994, 155.7920, 356.5939, 162.1916],\n",
       "         [177.7440, 167.4689, 181.1125, 174.3188],\n",
       "         [ 26.5268, 172.4662,  56.6747, 191.3297],\n",
       "         [ 72.3381, 172.2135,  84.6331, 176.4241],\n",
       "         [198.6561, 165.0555, 207.5826, 173.1399],\n",
       "         [182.3190, 167.4977, 207.7510, 186.8664],\n",
       "         [130.1918, 170.9504, 156.1290, 189.8138],\n",
       "         [215.2786, 162.4020, 230.8782, 173.7281],\n",
       "         [245.0990, 160.1294, 260.6776, 173.3501],\n",
       "         [271.0784, 157.9817, 278.6575, 160.3396],\n",
       "         [130.5286, 165.3924, 136.5919, 178.5294],\n",
       "         [234.3620, 160.7607, 240.9305, 165.9819]]),\n",
       " 'labels': tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0]),\n",
       " 'area': tensor([  37.4438,  632.3635,  438.9651,  133.3230,   75.3135,   59.9106,\n",
       "           59.9106,  229.7697,   61.2720,   83.6816,  174.6250,   24.3435,\n",
       "          492.0477,   37.6142,   24.1116,  128.6712,  421.7553, 1631.9044,\n",
       "           47.9622,   23.0740,  568.6947,   51.7690,   72.1647,  492.5867,\n",
       "          489.2675,  176.6827,  205.9615,   17.8710,   79.6535,   34.2952]),\n",
       " 'image_id': 'blob_755_frame_66.pickle'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "output = model([training_set[0][0]], [training_set[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(1.5879, grad_fn=<NllLossBackward>),\n",
       " 'loss_box_reg': tensor(0.0044, grad_fn=<DivBackward0>),\n",
       " 'loss_objectness': tensor(0.3741),\n",
       " 'loss_rpn_box_reg': tensor(1.0208)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter_per_epoch = len(training_generator)\n",
    "test_interval = 4\n",
    "num_epochs = 2\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    # if torch.cuda.is_available():\n",
    "    #     model.module.freeze_bn()\n",
    "    # else:\n",
    "    #     model.freeze_bn()\n",
    "    epoch_loss = []\n",
    "    progress_bar = tqdm(training_generator)\n",
    "    for iter, data in enumerate(progress_bar):\n",
    "        optimizer.zero_grad()\n",
    "        images = data[0]\n",
    "        targets = data[1]\n",
    "        images = list(image for image in images)\n",
    "        targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "#             losses = model([data['img'].cuda().float(), data['annot'].cuda()])\n",
    "#             losses = model(data[0].cuda(), data[1].cuda())\n",
    "            losses = model(images.cuda(), targets.cuda())\n",
    "            cls_loss, reg_loss = losses['loss_classifier'], losses['loss_box_reg']\n",
    "        else:\n",
    "#             losses = model([data['img'].float(), data['annot']])\n",
    "#             losses = model(data[0], data[1])\n",
    "            losses = model(images, targets)\n",
    "            cls_loss, reg_loss = losses['loss_classifier'], losses['loss_box_reg']\n",
    "\n",
    "        cls_loss = cls_loss.mean()\n",
    "        reg_loss = reg_loss.mean()\n",
    "        loss = cls_loss + reg_loss\n",
    "        if loss == 0:\n",
    "            continue\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(float(loss))\n",
    "        total_loss = np.mean(epoch_loss)\n",
    "        if iter % 5 == 0:\n",
    "            print(f'Total loss at iteration {iter}: {total_loss}')\n",
    "        progress_bar.set_description(\n",
    "            'Epoch: {}/{}. Iteration: {}/{}. Cls loss: {:.5f}. Reg loss: {:.5f}. Batch loss: {:.5f} Total loss: {:.5f}'.format(\n",
    "                epoch + 1, num_epochs, iter + 1, num_iter_per_epoch, cls_loss, reg_loss, loss,\n",
    "                total_loss))\n",
    "        # writer.add_scalar('Train/Total_loss', total_loss, epoch * num_iter_per_epoch + iter)\n",
    "        # writer.add_scalar('Train/Regression_loss', reg_loss, epoch * num_iter_per_epoch + iter)\n",
    "        # writer.add_scalar('Train/Classfication_loss (focal loss)', cls_loss, epoch * num_iter_per_epoch + iter)\n",
    "        # Save every 100 samples\n",
    "#             if iter % 200 ==0:\n",
    "#                 print(f\"Saving model at :{opt.saved_path}/effijklkjcientdet_waymo.pth\")\n",
    "#                 torch.save(model, os.path.join(opt.saved_path, \"effickjhghjkientdet_waymo.pth\"))\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 continue\n",
    "#     lr_scheduler.step(np.mean(epoch_loss))\n",
    "\n",
    "#     if epoch % test_interval == 0:\n",
    "#         model.eval()\n",
    "#         loss_regression_ls = []\n",
    "#         loss_classification_ls = []\n",
    "#         for iter, data in enumerate(test_generator):\n",
    "#             with torch.no_grad():\n",
    "#                 images = data[0]\n",
    "#                 targets = data[1]\n",
    "#                 images = list(image for image in images)\n",
    "#                 targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "                \n",
    "#                 if torch.cuda.is_available():\n",
    "#                     losses = model(images.cuda(), targets.cuda())\n",
    "#                     cls_loss, reg_loss = losses['loss_classifier'], losses['loss_box_reg']\n",
    "#                 else:\n",
    "#                     losses = model(images, targets)\n",
    "#                     cls_loss, reg_loss = losses['loss_classifier'], losses['loss_box_reg']\n",
    "\n",
    "#                 cls_loss = cls_loss.mean()\n",
    "#                 reg_loss = reg_loss.mean()\n",
    "\n",
    "#                 loss_classification_ls.append(float(cls_loss))\n",
    "#                 loss_regression_ls.append(float(reg_loss))\n",
    "\n",
    "#         cls_loss = np.mean(loss_classification_ls)\n",
    "#         reg_loss = np.mean(loss_regression_ls)\n",
    "#         loss = cls_loss + reg_loss\n",
    "\n",
    "#         print(\n",
    "#             'Epoch: {}/{}. Classification loss: {:1.5f}. Regression loss: {:1.5f}. Total loss: {:1.5f}'.format(\n",
    "#                 epoch + 1, num_epochs, cls_loss, reg_loss,\n",
    "#                 np.mean(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "    loss_regression_ls = []\n",
    "    loss_classification_ls = []\n",
    "    for iter, data in enumerate(test_generator):\n",
    "        with torch.no_grad():\n",
    "            images = data[0]\n",
    "            targets = data[1]\n",
    "            images = list(image for image in images)\n",
    "            targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                losses = model(images.cuda(), targets.cuda())\n",
    "                cls_loss, reg_loss = losses['loss_classifier'], losses['loss_box_reg']\n",
    "            else:\n",
    "                losses = model(images, targets)\n",
    "                cls_loss, reg_loss = losses['loss_classifier'], losses['loss_box_reg']\n",
    "\n",
    "            cls_loss = cls_loss.mean()\n",
    "            reg_loss = reg_loss.mean()\n",
    "\n",
    "            loss_classification_ls.append(float(cls_loss))\n",
    "            loss_regression_ls.append(float(reg_loss))\n",
    "\n",
    "    cls_loss = np.mean(loss_classification_ls)\n",
    "    reg_loss = np.mean(loss_regression_ls)\n",
    "    loss = cls_loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inference\n",
    "model.eval()\n",
    "x = [training_set[2][0]]\n",
    "predictions = model(x)           # Returns predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]['boxes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples to extract the layers and their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, layer) in model._modules.items():\n",
    "    #iteration over outer layers\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "for (name, layer) in resnet._modules.items():\n",
    "    #iteration over outer layers\n",
    "    print((name))\n",
    "\n",
    "# resnet._modules['layer1'][0]._modules['bn1'].weight.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "dataset = WaymoDataset(cameras=['FRONT'], heatmaps=False)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    " collate_fn=utils.collate_fn)\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use our dataset and defined transformations\n",
    "# dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "# dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:1])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-1:])\n",
    "\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "# assign 1 worker\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
