{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('models/efficientdet/')\n",
    "import src.model as mdl\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "from torchvision import transforms\n",
    "from src.dataset import CocoDataset, Resizer, Normalizer\n",
    "from src.dataset import WaymoDataset\n",
    "wd = WaymoDataset(cameras=['FRONT'],  transform=transforms.Compose([Normalizer(), Resizer()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train as train\n",
    "parser = argparse.ArgumentParser(\n",
    "    \"EfficientDet: Scalable and Efficient Object Detection implementation by Signatrix GmbH\")\n",
    "parser.add_argument(\"--image_size\", type=int, default=512, help=\"The common width and height for all images\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=8, help=\"The number of images per batch\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "parser.add_argument('--alpha', type=float, default=0.25)\n",
    "parser.add_argument('--gamma', type=float, default=1.5)\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
    "parser.add_argument(\"--test_interval\", type=int, default=1, help=\"Number of epoches between testing phases\")\n",
    "parser.add_argument(\"--es_min_delta\", type=float, default=0.0,\n",
    "                    help=\"Early stopping's parameter: minimum change loss to qualify as an improvement\")\n",
    "parser.add_argument(\"--es_patience\", type=int, default=0,\n",
    "                    help=\"Early stopping's parameter: number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique.\")\n",
    "parser.add_argument(\"--cam_view\", type=str, default=\"FRONT\", help=\"Camera view to train on [FRONT, SIDE_LEFT...]\")\n",
    "parser.add_argument(\"--log_path\", type=str, default=\"tensorboard/efficientdet_waymo\")\n",
    "parser.add_argument(\"--saved_path\", type=str, default=\"trained_models\")\n",
    "parser.add_argument(\"--train_depth\", type=int, default=2)\n",
    "parser.add_argument(\"--pretrained_model\", type=str, default=\"\", help=\"Path of pretrained model\")\n",
    "parser.add_argument(\"--backbone\", type=str, default=\"efficientnet-b7\")\n",
    "parser.add_argument(\"-f\", type=str, default=\"bam\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# argparser = argparse.ArgumentParser()\n",
    "# argparser.batch_size=1\n",
    "train.train(args)\n",
    "# train.get_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "ed = mdl.EfficientDet()\n",
    "image = wd[0]['img'].numpy()\n",
    "model(data['img'].cuda().permute(2, 0, 1).float().unsqueeze(dim=0))\n",
    "# inputs = img.view([1, 3, 1280, 1920])\n",
    "image_size=1280\n",
    "\n",
    "# image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "height, width = image.shape[:2]\n",
    "image = image.astype(np.float32) / 255\n",
    "image[:, :, 0] = (image[:, :, 0] - 0.485) / 0.229\n",
    "image[:, :, 1] = (image[:, :, 1] - 0.456) / 0.224\n",
    "image[:, :, 2] = (image[:, :, 2] - 0.406) / 0.225\n",
    "if height > width:\n",
    "    scale = image_size / height\n",
    "    resized_height = image_size\n",
    "    resized_width = int(width * scale)\n",
    "else:\n",
    "    scale = image_size / width\n",
    "    resized_height = int(height * scale)\n",
    "    resized_width = image_size\n",
    "\n",
    "image = cv2.resize(image, (resized_width, resized_height))\n",
    "\n",
    "new_image = np.zeros((image_size, image_size, 3))\n",
    "new_image[0:resized_height, 0:resized_width] = image\n",
    "new_image = np.transpose(new_image, (2, 0, 1))\n",
    "new_image = new_image[None, :, :, :]\n",
    "new_image = torch.Tensor(new_image)\n",
    "\n",
    "print(new_image.shape)\n",
    "c3, c4, c5 = ed.backbone_net(new_image)\n",
    "p3 = ed.conv3(c3)\n",
    "p4 = ed.conv4(c4)\n",
    "p5 = ed.conv5(c5)\n",
    "p6 = ed.conv6(c5)\n",
    "p7 = ed.conv7(p6)\n",
    "\n",
    "features = [p3, p4, p5, p6, p7]\n",
    "features = ed.bifpn(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def transfer_learning(model, train_depth=2):\n",
    "    mlist = next(m for m in model.children() if isinstance(m,torch.nn.modules.container.ModuleList))\n",
    "    print(f\"Num trainable params before:{count_parameters(mlist)}\")\n",
    "    layers_to_freeze = mlist[:-train_depth]\n",
    "    for l in layers_to_freeze:\n",
    "        freeze_params(l)\n",
    "    print(f\"Num trainable params after:{count_parameters(mlist)}\")\n",
    "    print(f\"Layers 0 to {len(layers_to_freeze)-1} frozen, only training on last {train_depth} layers\")\n",
    "\n",
    "def freeze_params(l):\n",
    "    for p in l.parameters():\n",
    "        if p.requires_grad:\n",
    "            p.requires_grad = False\n",
    "# transfer_learning(ed.backbone_net.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import WaymoDataset\n",
    "wd = WaymoDataset(heatmaps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'src.model.EfficientDet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'src.loss.FocalLoss' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'efficientnet_pytorch.model.EfficientNet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4499798"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "ed = torch.load('trained_models/signatrix_efficientdet_coco.pth',\n",
    "          map_location=torch.device('cpu')).module\n",
    "# ed = mdl.EfficientDet()\n",
    "# for i in range(15):\n",
    "# image = wd[3]['img']\n",
    "# x = ed(image.permute(2, 0, 1).float().unsqueeze(dim=0))\n",
    "# x[0].shape\n",
    "count_parameters(ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_detection_metric_ops() missing 10 required positional arguments: 'config', 'prediction_frame_id', 'prediction_bbox', 'prediction_type', 'prediction_score', 'prediction_overlap_nlz', 'ground_truth_frame_id', 'ground_truth_bbox', 'ground_truth_type', and 'ground_truth_difficulty'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b87257496c33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwaymo_open_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection_metrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_detection_metric_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_detection_metric_ops() missing 10 required positional arguments: 'config', 'prediction_frame_id', 'prediction_bbox', 'prediction_type', 'prediction_score', 'prediction_overlap_nlz', 'ground_truth_frame_id', 'ground_truth_bbox', 'ground_truth_type', and 'ground_truth_difficulty'"
     ]
    }
   ],
   "source": [
    "import waymo_open_dataset.metrics.python.detection_metrics as wod\n",
    "wod.get_detection_metric_ops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
